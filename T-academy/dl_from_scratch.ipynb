{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dl_from_scratch.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"J-1jjj0g3Uyd","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"gGDdd1i83wpO","colab_type":"text"},"cell_type":"markdown","source":["# util"]},{"metadata":{"id":"WHcadC2d3yWa","colab_type":"code","colab":{}},"cell_type":"code","source":["# coding: utf-8\n","import numpy as np\n","\n","\n","def smooth_curve(x):\n","    \"\"\"손실 함수의 그래프를 매끄럽게 하기 위해 사용\n","    \n","    참고：http://glowingpython.blogspot.jp/2012/02/convolution-with-numpy.html\n","    \"\"\"\n","    window_len = 11\n","    s = np.r_[x[window_len-1:0:-1], x, x[-1:-window_len:-1]]\n","    w = np.kaiser(window_len, 2)\n","    y = np.convolve(w/w.sum(), s, mode='valid')\n","    return y[5:len(y)-5]\n","\n","\n","def shuffle_dataset(x, t):\n","    \"\"\"데이터셋을 뒤섞는다.\n","\n","    Parameters\n","    ----------\n","    x : 훈련 데이터\n","    t : 정답 레이블\n","    \n","    Returns\n","    -------\n","    x, t : 뒤섞은 훈련 데이터와 정답 레이블\n","    \"\"\"\n","    permutation = np.random.permutation(x.shape[0])\n","    x = x[permutation,:] if x.ndim == 2 else x[permutation,:,:,:]\n","    t = t[permutation]\n","\n","    return x, t\n","\n","def conv_output_size(input_size, filter_size, stride=1, pad=0):\n","    return (input_size + 2*pad - filter_size) / stride + 1\n","\n","\n","def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n","    \"\"\"다수의 이미지를 입력받아 2차원 배열로 변환한다(평탄화).\n","    \n","    Parameters\n","    ----------\n","    input_data : 4차원 배열 형태의 입력 데이터(이미지 수, 채널 수, 높이, 너비)\n","    filter_h : 필터의 높이\n","    filter_w : 필터의 너비\n","    stride : 스트라이드\n","    pad : 패딩\n","    \n","    Returns\n","    -------\n","    col : 2차원 배열\n","    \"\"\"\n","    N, C, H, W = input_data.shape\n","    out_h = (H + 2*pad - filter_h)//stride + 1\n","    out_w = (W + 2*pad - filter_w)//stride + 1\n","\n","    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n","    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n","\n","    for y in range(filter_h):\n","        y_max = y + stride*out_h\n","        for x in range(filter_w):\n","            x_max = x + stride*out_w\n","            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n","\n","    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n","    return col\n","\n","\n","def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n","    \"\"\"(im2col과 반대) 2차원 배열을 입력받아 다수의 이미지 묶음으로 변환한다.\n","    \n","    Parameters\n","    ----------\n","    col : 2차원 배열(입력 데이터)\n","    input_shape : 원래 이미지 데이터의 형상（예：(10, 1, 28, 28)）\n","    filter_h : 필터의 높이\n","    filter_w : 필터의 너비\n","    stride : 스트라이드\n","    pad : 패딩\n","    \n","    Returns\n","    -------\n","    img : 변환된 이미지들\n","    \"\"\"\n","    N, C, H, W = input_shape\n","    out_h = (H + 2*pad - filter_h)//stride + 1\n","    out_w = (W + 2*pad - filter_w)//stride + 1\n","    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n","\n","    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n","    for y in range(filter_h):\n","        y_max = y + stride*out_h\n","        for x in range(filter_w):\n","            x_max = x + stride*out_w\n","            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n","\n","    return img[:, :, pad:H + pad, pad:W + pad]\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UX8L2WvA3c55","colab_type":"text"},"cell_type":"markdown","source":["# functions"]},{"metadata":{"id":"oKyzO5dZ3XeW","colab_type":"code","colab":{}},"cell_type":"code","source":["# coding: utf-8\n","import numpy as np\n","\n","\n","def identity_function(x):\n","    return x\n","\n","\n","def step_function(x):\n","    return np.array(x > 0, dtype=np.int)\n","\n","\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))    \n","\n","\n","def sigmoid_grad(x):\n","    return (1.0 - sigmoid(x)) * sigmoid(x)\n","    \n","\n","def relu(x):\n","    return np.maximum(0, x)\n","\n","\n","def relu_grad(x):\n","    grad = np.zeros(x)\n","    grad[x>=0] = 1\n","    return grad\n","    \n","\n","def softmax(x):\n","    if x.ndim == 2:\n","        x = x.T\n","        x = x - np.max(x, axis=0)\n","        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n","        return y.T \n","\n","    x = x - np.max(x) # 오버플로 대책\n","    return np.exp(x) / np.sum(np.exp(x))\n","\n","\n","def mean_squared_error(y, t):\n","    return 0.5 * np.sum((y-t)**2)\n","\n","\n","def cross_entropy_error_one(y, t):\n","    delta = 1e-7\n","    return -np.sum(t * np.log(y + delta))\n","\n","    \n","def cross_entropy_error(y, t):\n","    if y.ndim == 1:\n","        t = t.reshape(1, t.size)\n","        y = y.reshape(1, y.size)\n","        \n","    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n","    if t.size == y.size:\n","        t = t.argmax(axis=1)\n","             \n","    batch_size = y.shape[0]\n","    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n","\n","\n","def softmax_loss(X, t):\n","    y = softmax(X)\n","    return cross_entropy_error(y, t)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4ZIUvccs3j8u","colab_type":"text"},"cell_type":"markdown","source":["# gradient"]},{"metadata":{"id":"LGygYr303Xg6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"2f2dcb2b-72d2-4992-cbc9-390b2b678fd9","executionInfo":{"status":"ok","timestamp":1556187962896,"user_tz":-540,"elapsed":1124,"user":{"displayName":"한대희","photoUrl":"","userId":"17475105266749362233"}}},"cell_type":"code","source":["# coding: utf-8\n","import numpy as np\n","\n","def _numerical_gradient_1d(f, x):\n","    h = 1e-4 # 0.0001\n","    grad = np.zeros_like(x)\n","    \n","    for idx in range(x.size):\n","        tmp_val = x[idx]\n","        x[idx] = float(tmp_val) + h\n","        fxh1 = f(x) # f(x+h)\n","        \n","        x[idx] = tmp_val - h \n","        fxh2 = f(x) # f(x-h)\n","        grad[idx] = (fxh1 - fxh2) / (2*h)\n","        \n","        x[idx] = tmp_val # 값 복원\n","        \n","    return grad\n","\n","\n","def numerical_gradient_2d(f, X):\n","    if X.ndim == 1:\n","        return _numerical_gradient_1d(f, X)\n","    else:\n","        grad = np.zeros_like(X)\n","        \n","        for idx, x in enumerate(X):\n","            grad[idx] = _numerical_gradient_1d(f, x)\n","        \n","        return grad\n","\n","\n","def numerical_gradient(f, x):\n","    h = 1e-4 # 0.0001\n","    grad = np.zeros_like(x)\n","    \n","    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n","    while not it.finished:\n","        idx = it.multi_index\n","        tmp_val = x[idx]\n","        x[idx] = float(tmp_val) + h\n","        fxh1 = f(x) # f(x+h)\n","        \n","        x[idx] = tmp_val - h \n","        fxh2 = f(x) # f(x-h)\n","        grad[idx] = (fxh1 - fxh2) / (2*h)\n","        \n","        x[idx] = tmp_val # 값 복원\n","        it.iternext()   \n","        \n","    return grad\n","\n","if __name__ == '__main__':\n","    f_xx = lambda x: x*x\n","    print(_numerical_gradient_1d(f_xx, np.array([3])))"],"execution_count":3,"outputs":[{"output_type":"stream","text":["[25000]\n"],"name":"stdout"}]},{"metadata":{"id":"jeHPPumc3o7y","colab_type":"text"},"cell_type":"markdown","source":["# layers"]},{"metadata":{"id":"jwnznSNZ3Xi8","colab_type":"code","colab":{}},"cell_type":"code","source":["# coding: utf-8\n","import numpy as np\n","\n","\n","class Relu:\n","    def __init__(self):\n","        self.mask = None\n","\n","    def forward(self, x):\n","        self.mask = (x <= 0)\n","        out = x.copy()\n","        out[self.mask] = 0\n","\n","        return out\n","\n","    def backward(self, dout):\n","        dout[self.mask] = 0\n","        dx = dout\n","\n","        return dx\n","\n","\n","class Sigmoid:\n","    def __init__(self):\n","        self.out = None\n","\n","    def forward(self, x):\n","        out = sigmoid(x)\n","        self.out = out\n","        return out\n","\n","    def backward(self, dout):\n","        dx = dout * (1.0 - self.out) * self.out\n","\n","        return dx\n","\n","\n","class Affine:\n","    def __init__(self, W, b):\n","        self.W = W\n","        self.b = b\n","        \n","        self.x = None\n","        self.original_x_shape = None\n","        # 가중치와 편향 매개변수의 미분\n","        self.dW = None\n","        self.db = None\n","\n","    def forward(self, x):\n","        # 텐서 대응\n","        self.original_x_shape = x.shape\n","        x = x.reshape(x.shape[0], -1)\n","        self.x = x\n","\n","        out = np.dot(self.x, self.W) + self.b\n","\n","        return out\n","\n","    def backward(self, dout):\n","        dx = np.dot(dout, self.W.T)\n","        self.dW = np.dot(self.x.T, dout)\n","        self.db = np.sum(dout, axis=0)\n","        \n","        dx = dx.reshape(*self.original_x_shape)  # 입력 데이터 모양 변경(텐서 대응)\n","        return dx\n","\n","\n","class SoftmaxWithLoss:\n","    def __init__(self):\n","        self.loss = None # 손실함수\n","        self.y = None    # softmax의 출력\n","        self.t = None    # 정답 레이블(원-핫 인코딩 형태)\n","        \n","    def forward(self, x, t):\n","        self.t = t\n","        self.y = softmax(x)\n","        self.loss = cross_entropy_error(self.y, self.t)\n","        \n","        return self.loss\n","\n","    def backward(self, dout=1):\n","        batch_size = self.t.shape[0]\n","        if self.t.size == self.y.size: # 정답 레이블이 원-핫 인코딩 형태일 때\n","            dx = (self.y - self.t) / batch_size\n","        else:\n","            dx = self.y.copy()\n","            dx[np.arange(batch_size), self.t] -= 1\n","            dx = dx / batch_size\n","        \n","        return dx\n","\n","\n","class Dropout:\n","    \"\"\"\n","    http://arxiv.org/abs/1207.0580\n","    \"\"\"\n","    def __init__(self, dropout_ratio=0.5):\n","        self.dropout_ratio = dropout_ratio\n","        self.mask = None\n","\n","    def forward(self, x, train_flg=True):\n","        if train_flg:\n","            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n","            return x * self.mask\n","        else:\n","            return x * (1.0 - self.dropout_ratio)\n","\n","    def backward(self, dout):\n","        return dout * self.mask\n","\n","\n","class BatchNormalization:\n","    \"\"\"\n","    http://arxiv.org/abs/1502.03167\n","    \"\"\"\n","    def __init__(self, gamma, beta, momentum=0.9, running_mean=None, running_var=None):\n","        self.gamma = gamma\n","        self.beta = beta\n","        self.momentum = momentum\n","        self.input_shape = None # 합성곱 계층은 4차원, 완전연결 계층은 2차원  \n","\n","        # 시험할 때 사용할 평균과 분산\n","        self.running_mean = running_mean\n","        self.running_var = running_var  \n","        \n","        # backward 시에 사용할 중간 데이터\n","        self.batch_size = None\n","        self.xc = None\n","        self.std = None\n","        self.dgamma = None\n","        self.dbeta = None\n","\n","    def forward(self, x, train_flg=True):\n","        self.input_shape = x.shape\n","        if x.ndim != 2:\n","            N, C, H, W = x.shape\n","            x = x.reshape(N, -1)\n","\n","        out = self.__forward(x, train_flg)\n","        \n","        return out.reshape(*self.input_shape)\n","            \n","    def __forward(self, x, train_flg):\n","        if self.running_mean is None:\n","            N, D = x.shape\n","            self.running_mean = np.zeros(D)\n","            self.running_var = np.zeros(D)\n","                        \n","        if train_flg:\n","            mu = x.mean(axis=0)\n","            xc = x - mu\n","            var = np.mean(xc**2, axis=0)\n","            std = np.sqrt(var + 10e-7)\n","            xn = xc / std\n","            \n","            self.batch_size = x.shape[0]\n","            self.xc = xc\n","            self.xn = xn\n","            self.std = std\n","            self.running_mean = self.momentum * self.running_mean + (1-self.momentum) * mu\n","            self.running_var = self.momentum * self.running_var + (1-self.momentum) * var            \n","        else:\n","            xc = x - self.running_mean\n","            xn = xc / ((np.sqrt(self.running_var + 10e-7)))\n","            \n","        out = self.gamma * xn + self.beta \n","        return out\n","\n","    def backward(self, dout):\n","        if dout.ndim != 2:\n","            N, C, H, W = dout.shape\n","            dout = dout.reshape(N, -1)\n","\n","        dx = self.__backward(dout)\n","\n","        dx = dx.reshape(*self.input_shape)\n","        return dx\n","\n","    def __backward(self, dout):\n","        dbeta = dout.sum(axis=0)\n","        dgamma = np.sum(self.xn * dout, axis=0)\n","        dxn = self.gamma * dout\n","        dxc = dxn / self.std\n","        dstd = -np.sum((dxn * self.xc) / (self.std * self.std), axis=0)\n","        dvar = 0.5 * dstd / self.std\n","        dxc += (2.0 / self.batch_size) * self.xc * dvar\n","        dmu = np.sum(dxc, axis=0)\n","        dx = dxc - dmu / self.batch_size\n","        \n","        self.dgamma = dgamma\n","        self.dbeta = dbeta\n","        \n","        return dx\n","\n","\n","class Convolution:\n","    def __init__(self, W, b, stride=1, pad=0):\n","        self.W = W\n","        self.b = b\n","        self.stride = stride\n","        self.pad = pad\n","        \n","        # 중간 데이터（backward 시 사용）\n","        self.x = None   \n","        self.col = None\n","        self.col_W = None\n","        \n","        # 가중치와 편향 매개변수의 기울기\n","        self.dW = None\n","        self.db = None\n","\n","    def forward(self, x):\n","        FN, C, FH, FW = self.W.shape\n","        N, C, H, W = x.shape\n","        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n","        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n","\n","        col = im2col(x, FH, FW, self.stride, self.pad)\n","        col_W = self.W.reshape(FN, -1).T\n","\n","        out = np.dot(col, col_W) + self.b\n","        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n","\n","        self.x = x\n","        self.col = col\n","        self.col_W = col_W\n","\n","        return out\n","\n","    def backward(self, dout):\n","        FN, C, FH, FW = self.W.shape\n","        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n","\n","        self.db = np.sum(dout, axis=0)\n","        self.dW = np.dot(self.col.T, dout)\n","        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n","\n","        dcol = np.dot(dout, self.col_W.T)\n","        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n","\n","        return dx\n","\n","\n","class Pooling:\n","    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n","        self.pool_h = pool_h\n","        self.pool_w = pool_w\n","        self.stride = stride\n","        self.pad = pad\n","        \n","        self.x = None\n","        self.arg_max = None\n","\n","    def forward(self, x):\n","        N, C, H, W = x.shape\n","        out_h = int(1 + (H - self.pool_h) / self.stride)\n","        out_w = int(1 + (W - self.pool_w) / self.stride)\n","\n","        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n","        col = col.reshape(-1, self.pool_h*self.pool_w)\n","\n","        arg_max = np.argmax(col, axis=1)\n","        out = np.max(col, axis=1)\n","        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n","\n","        self.x = x\n","        self.arg_max = arg_max\n","\n","        return out\n","\n","    def backward(self, dout):\n","        dout = dout.transpose(0, 2, 3, 1)\n","        \n","        pool_size = self.pool_h * self.pool_w\n","        dmax = np.zeros((dout.size, pool_size))\n","        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n","        dmax = dmax.reshape(dout.shape + (pool_size,)) \n","        \n","        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n","        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n","        \n","        return dx\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xYSOykfQ6A1r","colab_type":"text"},"cell_type":"markdown","source":["# two layer net"]},{"metadata":{"id":"LebS2w-J3XlV","colab_type":"code","colab":{}},"cell_type":"code","source":["# coding: utf-8\n","import sys, os\n","sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n","\n","\n","\n","class TwoLayerNet:\n","\n","    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n","        # 가중치 초기화\n","        self.params = {}\n","        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n","        self.params['b1'] = np.zeros(hidden_size)\n","        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n","        self.params['b2'] = np.zeros(output_size)\n","\n","    def predict(self, x):\n","        W1, W2 = self.params['W1'], self.params['W2']\n","        b1, b2 = self.params['b1'], self.params['b2']\n","    \n","        a1 = np.dot(x, W1) + b1\n","        z1 = sigmoid(a1)\n","        a2 = np.dot(z1, W2) + b2\n","        y = softmax(a2)\n","        \n","        return y\n","        \n","    # x : 입력 데이터, t : 정답 레이블\n","    def loss(self, x, t):\n","        y = self.predict(x)\n","        \n","        return cross_entropy_error(y, t)\n","    \n","    def accuracy(self, x, t):\n","        y = self.predict(x)\n","        y = np.argmax(y, axis=1)\n","        t = np.argmax(t, axis=1)\n","        \n","        accuracy = np.sum(y == t) / float(x.shape[0])\n","        return accuracy\n","        \n","    # x : 입력 데이터, t : 정답 레이블\n","    def numerical_gradient(self, x, t):\n","        loss_W = lambda W: self.loss(x, t)\n","        \n","        grads = {}\n","        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n","        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n","        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n","        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n","        \n","        return grads\n","        \n","    def gradient(self, x, t):\n","        W1, W2 = self.params['W1'], self.params['W2']\n","        b1, b2 = self.params['b1'], self.params['b2']\n","        grads = {}\n","        \n","        batch_num = x.shape[0]\n","        \n","        # forward\n","        a1 = np.dot(x, W1) + b1\n","        z1 = sigmoid(a1)\n","        a2 = np.dot(z1, W2) + b2\n","        y = softmax(a2)\n","        \n","        # backward\n","        dy = (y - t) / batch_num\n","        grads['W2'] = np.dot(z1.T, dy)\n","        grads['b2'] = np.sum(dy, axis=0)\n","        \n","        da1 = np.dot(dy, W2.T)\n","        dz1 = sigmoid_grad(a1) * da1\n","        grads['W1'] = np.dot(x.T, dz1)\n","        grads['b1'] = np.sum(dz1, axis=0)\n","\n","        return grads\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"balDXFgH4Adv","colab_type":"text"},"cell_type":"markdown","source":["# multi layer"]},{"metadata":{"id":"etIpDuWB6Hj-","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"EYreNfT34A9h","colab_type":"code","colab":{}},"cell_type":"code","source":["# coding: utf-8\n","import sys, os\n","sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n","import numpy as np\n","from collections import OrderedDict\n","\n","\n","class MultiLayerNet:\n","    \"\"\"완전연결 다층 신경망\n","\n","    Parameters\n","    ----------\n","    input_size : 입력 크기（MNIST의 경우엔 784）\n","    hidden_size_list : 각 은닉층의 뉴런 수를 담은 리스트（e.g. [100, 100, 100]）\n","    output_size : 출력 크기（MNIST의 경우엔 10）\n","    activation : 활성화 함수 - 'relu' 혹은 'sigmoid'\n","    weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\n","        'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\n","        'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\n","    weight_decay_lambda : 가중치 감소(L2 법칙)의 세기\n","    \"\"\"\n","    def __init__(self, input_size, hidden_size_list, output_size,\n","                 activation='relu', weight_init_std='relu', weight_decay_lambda=0):\n","        self.input_size = input_size\n","        self.output_size = output_size\n","        self.hidden_size_list = hidden_size_list\n","        self.hidden_layer_num = len(hidden_size_list)\n","        self.weight_decay_lambda = weight_decay_lambda\n","        self.params = {}\n","\n","        # 가중치 초기화\n","        self.__init_weight(weight_init_std)\n","\n","        # 계층 생성\n","        activation_layer = {'sigmoid': Sigmoid, 'relu': Relu}\n","        self.layers = OrderedDict()\n","        for idx in range(1, self.hidden_layer_num+1):\n","            self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)],\n","                                                      self.params['b' + str(idx)])\n","            self.layers['Activation_function' + str(idx)] = activation_layer[activation]()\n","\n","        idx = self.hidden_layer_num + 1\n","        self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)],\n","            self.params['b' + str(idx)])\n","\n","        self.last_layer = SoftmaxWithLoss()\n","\n","    def __init_weight(self, weight_init_std):\n","        \"\"\"가중치 초기화\n","        \n","        Parameters\n","        ----------\n","        weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\n","            'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\n","            'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\n","        \"\"\"\n","        all_size_list = [self.input_size] + self.hidden_size_list + [self.output_size]\n","        for idx in range(1, len(all_size_list)):\n","            scale = weight_init_std\n","            if str(weight_init_std).lower() in ('relu', 'he'):\n","                scale = np.sqrt(2.0 / all_size_list[idx - 1])  # ReLU를 사용할 때의 권장 초깃값\n","            elif str(weight_init_std).lower() in ('sigmoid', 'xavier'):\n","                scale = np.sqrt(1.0 / all_size_list[idx - 1])  # sigmoid를 사용할 때의 권장 초깃값\n","            self.params['W' + str(idx)] = scale * np.random.randn(all_size_list[idx-1], all_size_list[idx])\n","            self.params['b' + str(idx)] = np.zeros(all_size_list[idx])\n","\n","    def predict(self, x):\n","        for layer in self.layers.values():\n","            x = layer.forward(x)\n","\n","        return x\n","\n","    def loss(self, x, t):\n","        \"\"\"손실 함수를 구한다.\n","        \n","        Parameters\n","        ----------\n","        x : 입력 데이터\n","        t : 정답 레이블 \n","        \n","        Returns\n","        -------\n","        손실 함수의 값\n","        \"\"\"\n","        y = self.predict(x)\n","\n","        weight_decay = 0\n","        for idx in range(1, self.hidden_layer_num + 2):\n","            W = self.params['W' + str(idx)]\n","            weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W ** 2)\n","\n","        return self.last_layer.forward(y, t) + weight_decay\n","\n","    def accuracy(self, x, t):\n","        y = self.predict(x)\n","        y = np.argmax(y, axis=1)\n","        if t.ndim != 1 : t = np.argmax(t, axis=1)\n","\n","        accuracy = np.sum(y == t) / float(x.shape[0])\n","        return accuracy\n","\n","    def numerical_gradient(self, x, t):\n","        \"\"\"기울기를 구한다(수치 미분).\n","        \n","        Parameters\n","        ----------\n","        x : 입력 데이터\n","        t : 정답 레이블\n","        \n","        Returns\n","        -------\n","        각 층의 기울기를 담은 딕셔너리(dictionary) 변수\n","            grads['W1']、grads['W2']、... 각 층의 가중치\n","            grads['b1']、grads['b2']、... 각 층의 편향\n","        \"\"\"\n","        loss_W = lambda W: self.loss(x, t)\n","\n","        grads = {}\n","        for idx in range(1, self.hidden_layer_num+2):\n","            grads['W' + str(idx)] = numerical_gradient(loss_W, self.params['W' + str(idx)])\n","            grads['b' + str(idx)] = numerical_gradient(loss_W, self.params['b' + str(idx)])\n","\n","        return grads\n","\n","    def gradient(self, x, t):\n","        \"\"\"기울기를 구한다(오차역전파법).\n","\n","        Parameters\n","        ----------\n","        x : 입력 데이터\n","        t : 정답 레이블\n","        \n","        Returns\n","        -------\n","        각 층의 기울기를 담은 딕셔너리(dictionary) 변수\n","            grads['W1']、grads['W2']、... 각 층의 가중치\n","            grads['b1']、grads['b2']、... 각 층의 편향\n","        \"\"\"\n","        # forward\n","        self.loss(x, t)\n","\n","        # backward\n","        dout = 1\n","        dout = self.last_layer.backward(dout)\n","\n","        layers = list(self.layers.values())\n","        layers.reverse()\n","        for layer in layers:\n","            dout = layer.backward(dout)\n","\n","        # 결과 저장\n","        grads = {}\n","        for idx in range(1, self.hidden_layer_num+2):\n","            grads['W' + str(idx)] = self.layers['Affine' + str(idx)].dW + self.weight_decay_lambda * self.layers['Affine' + str(idx)].W\n","            grads['b' + str(idx)] = self.layers['Affine' + str(idx)].db\n","\n","        return grads\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OS92B8OO4DZF","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"e3WcbkKK4h4F","colab_type":"text"},"cell_type":"markdown","source":["# simple net"]},{"metadata":{"id":"PY_6swZq3XnQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"outputId":"5beae8ed-4688-4c61-e483-3088b64ec9f6","executionInfo":{"status":"ok","timestamp":1556187964076,"user_tz":-540,"elapsed":2250,"user":{"displayName":"한대희","photoUrl":"","userId":"17475105266749362233"}}},"cell_type":"code","source":["# coding: utf-8\n","import sys, os\n","sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n","import numpy as np\n","\n","class simpleNet:\n","    def __init__(self):\n","        self.W = np.random.randn(2,3) # 정규분포로 초기화\n","\n","    def predict(self, x):\n","        return np.dot(x, self.W)\n","\n","    def loss(self, x, t):\n","        z = self.predict(x)\n","        y = softmax(z)\n","        loss = cross_entropy_error(y, t)\n","\n","        return loss\n","\n","x = np.array([0.6, 0.9])\n","t = np.array([0, 0, 1])\n","\n","net = simpleNet()\n","\n","f = lambda w: net.loss(x, t)\n","dW = numerical_gradient(f, net.W)\n","\n","print(dW)\n"],"execution_count":7,"outputs":[{"output_type":"stream","text":["[[ 0.15053442  0.14911474 -0.29964916]\n"," [ 0.22580164  0.22367211 -0.44947375]]\n"],"name":"stdout"}]},{"metadata":{"id":"Lkf0jTOB6o4e","colab_type":"text"},"cell_type":"markdown","source":["# data 만들기"]},{"metadata":{"id":"-oPMMM5r6qyM","colab_type":"code","colab":{}},"cell_type":"code","source":["import math\n","import random\n","import matplotlib.pyplot as plt\n","\n","\n","# y = w * x + b 함수\n","def myfunc(x):\n","  y = math.cos(x) + math.sin(x) * math.sin(x)\n","  noise = random.random() * 0.1    # Noise\n","  return y + noise"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qUOUElPc7Hoi","colab_type":"code","colab":{}},"cell_type":"code","source":["# random.random() -- (0.0 ~ 1.0)\n","NUM_DATA = 50      # 데이터 갯수\n","XVALUE = 5          # X값의 범위 (0.0 ~ 5.0)\n","\n","x_train = [random.random() * XVALUE for i in range(NUM_DATA)]\n","x_train.sort()        # sorting\n","t_train = [myfunc(x) for x in x_train]\n","\n","x_test = [random.random() * XVALUE for i in range(50)]\n","t_test = [myfunc(x) for x in x_test]\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xUpEpEzS7IZi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":269},"outputId":"d1ccb176-db05-4d9f-d4f5-61f960fcb837","executionInfo":{"status":"ok","timestamp":1556187964906,"user_tz":-540,"elapsed":3062,"user":{"displayName":"한대희","photoUrl":"","userId":"17475105266749362233"}}},"cell_type":"code","source":["fig, ax = plt.subplots()\n","ax.scatter(x_train, t_train, label='actual')\n","plt.show()"],"execution_count":10,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"JL69a0Tt6JHj","colab_type":"text"},"cell_type":"markdown","source":["# train two layer net"]},{"metadata":{"id":"W9jxE_8i6JZU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1627},"outputId":"bd2227fa-2a36-4e8f-b435-b83667e451af","executionInfo":{"status":"ok","timestamp":1556188116903,"user_tz":-540,"elapsed":1714,"user":{"displayName":"한대희","photoUrl":"","userId":"17475105266749362233"}}},"cell_type":"code","source":["# coding: utf-8\n","import sys, os\n","sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# 데이터 읽기\n","#(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n","\n","x_train, t_train = np.array(x_train).reshape(-1, 1), np.array(t_train).reshape(-1, 1)\n","x_test, t_test = np.array(x_test).reshape(-1, 1), np.array(t_test).reshape(-1, 1)\n","\n","\n","network = TwoLayerNet(input_size=1, hidden_size=2, output_size=1)\n","\n","# 하이퍼파라미터\n","iters_num = 200  # 반복 횟수를 적절히 설정한다.\n","train_size = x_train.shape[0]\n","batch_size = 10   # 미니배치 크기\n","learning_rate = 0.05\n","\n","train_loss_list = []\n","train_acc_list = []\n","test_acc_list = []\n","\n","# 1에폭당 반복 수\n","iter_per_epoch = max(train_size / batch_size, 1)\n","\n","for i in range(iters_num):\n","    # 미니배치 획득\n","    batch_mask = np.random.choice(train_size, batch_size)\n","    x_batch = x_train[batch_mask]\n","    t_batch = t_train[batch_mask]\n","    \n","    # 기울기 계산\n","    grad = network.numerical_gradient(x_batch, t_batch)\n","    #grad = network.gradient(x_batch, t_batch)\n","    \n","    # 매개변수 갱신\n","    for key in ('W1', 'b1', 'W2', 'b2'):\n","        network.params[key] -= learning_rate * grad[key]\n","    \n","    # 학습 경과 기록\n","    loss = network.loss(x_batch, t_batch)\n","    train_loss_list.append(loss)\n","    \n","    # 1에폭당 정확도 계산\n","    if i % iter_per_epoch == 0:\n","        train_acc = network.accuracy(x_train, t_train)\n","        test_acc = network.accuracy(x_test, t_test)\n","        train_acc_list.append(train_acc)\n","        test_acc_list.append(test_acc)\n","        print(loss)\n","        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n","\n","# 그래프 그리기\n","markers = {'train': 'o', 'test': 's'}\n","x = np.arange(len(train_acc_list))\n","plt.plot(x, train_acc_list, label='train acc')\n","plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n","plt.xlabel(\"epochs\")\n","plt.ylabel(\"accuracy\")\n","plt.ylim(0, 1.0)\n","plt.legend(loc='lower right')\n","plt.show()\n"],"execution_count":13,"outputs":[{"output_type":"stream","text":["-9.999999505838704e-08\n","train acc, test acc | 1.0, 1.0\n","-9.999999505838704e-08\n","train acc, test acc | 1.0, 1.0\n","-9.999999505838704e-08\n","train acc, test acc | 1.0, 1.0\n","-9.999999505838704e-08\n","train acc, test acc | 1.0, 1.0\n","-9.999999505838704e-08\n","train acc, test acc | 1.0, 1.0\n","-9.999999505838704e-08\n","train acc, test acc | 1.0, 1.0\n","-9.999999505838704e-08\n","train acc, test acc | 1.0, 1.0\n","-9.999999505838704e-08\n","train acc, test acc | 1.0, 1.0\n","-9.999999505838704e-08\n","train acc, test acc | 1.0, 1.0\n","-9.999999505838704e-08\n","train acc, test acc | 1.0, 1.0\n","-9.999999505838704e-08\n","train acc, test acc | 1.0, 1.0\n","-9.999999505838704e-08\n","train acc, test acc | 1.0, 1.0\n","-9.999999505838704e-08\n","train acc, test acc | 1.0, 1.0\n","-9.999999505838704e-08\n","train acc, test acc | 1.0, 1.0\n","-9.999999505838704e-08\n","train acc, test acc | 1.0, 1.0\n","-9.999999505838704e-08\n","train acc, test acc | 1.0, 1.0\n","-9.999999505838704e-08\n","train acc, test acc | 1.0, 1.0\n","-9.999999505838704e-08\n","train acc, test acc | 1.0, 1.0\n","-9.999999505838704e-08\n","train acc, test acc | 1.0, 1.0\n","-9.999999505838704e-08\n","train acc, test acc | 1.0, 1.0\n","-9.999999505838704e-08\n","train acc, test acc | 1.0, 1.0\n","-9.999999505838704e-08\n","train acc, test acc | 1.0, 1.0\n","-9.999999505838704e-08\n","train acc, test acc | 1.0, 1.0\n","-9.999999505838704e-08\n","train acc, test acc | 1.0, 1.0\n","-9.999999505838704e-08\n","train acc, test acc | 1.0, 1.0\n","-9.999999505838704e-08\n","train acc, test acc | 1.0, 1.0\n","-9.999999505838704e-08\n","train acc, test acc | 1.0, 1.0\n","-9.999999505838704e-08\n","train acc, test acc | 1.0, 1.0\n","-9.999999505838704e-08\n","train acc, test acc | 1.0, 1.0\n","-9.999999505838704e-08\n","train acc, test acc | 1.0, 1.0\n","-9.999999505838704e-08\n","train acc, test acc | 1.0, 1.0\n","-9.999999505838704e-08\n","train acc, test acc | 1.0, 1.0\n","-9.999999505838704e-08\n","train acc, test acc | 1.0, 1.0\n","-9.999999505838704e-08\n","train acc, test acc | 1.0, 1.0\n","-9.999999505838704e-08\n","train acc, test acc | 1.0, 1.0\n","-9.999999505838704e-08\n","train acc, test acc | 1.0, 1.0\n","-9.999999505838704e-08\n","train acc, test acc | 1.0, 1.0\n","-9.999999505838704e-08\n","train acc, test acc | 1.0, 1.0\n","-9.999999505838704e-08\n","train acc, test acc | 1.0, 1.0\n","-9.999999505838704e-08\n","train acc, test acc | 1.0, 1.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"SMOBG6IfCOA6","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}